{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzP_BJJ170v6"
   },
   "source": [
    "# Lab 2 Exploration: Color Images\n",
    "\n",
    "In this notebook, we will learn how to use the racecar camera to identify objects based on their color and extract the center and area of these objects.\n",
    "\n",
    "Throughout this notebook, **<font style=\"color:red\">text in bold red</font>** indicates a change you must make to the following code block before running it.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Getting Started](#GettingStarted)\n",
    "1. [Taking Photos](#TakingPhotos)\n",
    "1. [Color Formats](#ColorFormats)\n",
    "1. [Masks](#Masks)\n",
    "1. [Finding Contours](#FindingContours)\n",
    "1. [Contour Center](#ContourCenter)\n",
    "1. [Contour Area](#ContourArea)\n",
    "1. [Processing Numeric Values](#ProcessingNumericValues)\n",
    "1. [Proportional Control](#ProportionalControl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Dta7TQK70v_"
   },
   "source": [
    "<a id=\"ROS2 Node Setup\"></a>\n",
    "## 0. ROS2 Node Setup\n",
    "\n",
    "Run the following two scripts to set up the ROS2 subscriber node that retrieves color images. After the first time the scripts have been run, they must not be run again, else the system will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NYOyEdb670wA"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rclpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Import Asynchronous ROS2 initialization script\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrclpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mros2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrclpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mqos\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     QoSDurabilityPolicy,\n\u001b[1;32m      5\u001b[0m     QoSHistoryPolicy,\n\u001b[1;32m      6\u001b[0m     QoSReliabilityPolicy,\n\u001b[1;32m      7\u001b[0m     QoSProfile,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msensor_msgs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmsg\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rclpy'"
     ]
    }
   ],
   "source": [
    "# Import Asynchronous ROS2 initialization script\n",
    "import rclpy as ros2\n",
    "from rclpy.qos import (\n",
    "    QoSDurabilityPolicy,\n",
    "    QoSHistoryPolicy,\n",
    "    QoSReliabilityPolicy,\n",
    "    QoSProfile,\n",
    ")\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "\n",
    "# Create node\n",
    "ros2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "op4rdIFH70wC"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from nptyping import NDArray\n",
    "\n",
    "class CameraReal():\n",
    "    # The ROS topic from which we read camera data\n",
    "    __COLOR_TOPIC = \"/camera/color\"\n",
    "    __DEPTH_TOPIC = \"/camera/depth\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__bridge = CvBridge()\n",
    "\n",
    "        # ROS node\n",
    "        self.node = ros2.create_node(\"image_sub\")\n",
    "\n",
    "        qos_profile = QoSProfile(depth=1)\n",
    "        qos_profile.history = QoSHistoryPolicy.RMW_QOS_POLICY_HISTORY_KEEP_LAST\n",
    "        qos_profile.reliability = (\n",
    "            QoSReliabilityPolicy.RMW_QOS_POLICY_RELIABILITY_BEST_EFFORT\n",
    "        )\n",
    "        qos_profile.durability = QoSDurabilityPolicy.RMW_QOS_POLICY_DURABILITY_VOLATILE\n",
    "\n",
    "        # subscribe to the color image topic, which will call\n",
    "        # __color_callback every time the camera publishes data\n",
    "        self.__color_image_sub = self.node.create_subscription(\n",
    "            Image, self.__COLOR_TOPIC, self.__color_callback, qos_profile\n",
    "        )\n",
    "        self.__color_image = None\n",
    "        self.__color_image_new = None\n",
    "\n",
    "        # subscribe to the depth image topic, which will call\n",
    "        # __depth_callback every time the camera publishes data\n",
    "        self.__depth_image_sub = self.node.create_subscription(\n",
    "            Image, self.__DEPTH_TOPIC, self.__depth_callback, qos_profile\n",
    "        )\n",
    "        self.__depth_image = None\n",
    "        self.__depth_image_new = None\n",
    "\n",
    "    def __color_callback(self, data):\n",
    "        try:\n",
    "            np_arr = np.frombuffer(data.data, np.uint8) # decode jpeg image type\n",
    "            cv_color_image = cv.imdecode(np_arr, cv.IMREAD_COLOR)\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "\n",
    "        self.__color_image_new = cv_color_image\n",
    "\n",
    "    def __depth_callback(self, data):\n",
    "        try:\n",
    "            cv_depth_image = self.__bridge.imgmsg_to_cv2(data, desired_encoding=\"16UC1\")\n",
    "        except CvBridgeError as e:\n",
    "            print(e)\n",
    "\n",
    "        self.__depth_image_new = cv_depth_image\n",
    "\n",
    "    def get_color_image_no_copy(self) -> NDArray[(480, 640, 3), np.uint8]:\n",
    "        return self.__color_image\n",
    "\n",
    "    def get_depth_image(self) -> NDArray[(480, 640), np.float32]:\n",
    "        return self.__depth_image\n",
    "\n",
    "    def get_color_image_async(self) -> NDArray[(480, 640, 3), np.uint8]:\n",
    "        ros2.spin_once(self.node)\n",
    "        self.__color_image = self.__color_image_new\n",
    "        return self.__color_image_new\n",
    "\n",
    "    def get_depth_image_async(self) -> NDArray[(480, 640), np.float32]:\n",
    "        ros2.spin_once(self.node)\n",
    "        self.__depth_image = self.__depth_image_new\n",
    "        return self.__depth_image_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzf0nHa170wD"
   },
   "source": [
    "<a id=\"GettingStarted\"></a>\n",
    "## 1. Getting Started\n",
    "\n",
    "**<font style=\"color:red\">If you are running the car in RacecarSim, set `isSimulation` to `True`</font>**. Leave `isSimulation` `False` if you are using a physical car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gkkTTJoK70wE"
   },
   "outputs": [],
   "source": [
    "# TODO: Update isSimulation if necessary\n",
    "isSimulation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYHxuHIr70wE"
   },
   "source": [
    "Next, we will import the necessary libraries for this notebook, including Python libraries (`cv`, `numpy`, etc.) and the Racecar library (`racecar_core`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nq_sABxA70wF"
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from nptyping import NDArray\n",
    "from typing import Any, Tuple, List, Optional\n",
    "\n",
    "# Import Racecar library\n",
    "import sys\n",
    "sys.path.append(\"../../library\")\n",
    "import racecar_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZpPx6-T70wG"
   },
   "source": [
    "The following functions will help us throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4LHQBcrA70wH"
   },
   "outputs": [],
   "source": [
    "def show_image(image: NDArray) -> None:\n",
    "    \"\"\"\n",
    "    Displays a color image in the Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    plt.imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_contour(\n",
    "    image: NDArray,\n",
    "    contour: NDArray,\n",
    "    color: Tuple[int, int, int] = (0, 255, 0)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draws a contour on the provided image.\n",
    "\n",
    "    Args:\n",
    "        image: The image on which to draw the contour.\n",
    "        contour: The contour to draw on the image.\n",
    "        color: The color to draw the contour in BGR format.\n",
    "    \"\"\"\n",
    "    cv.drawContours(image_copy, [contour], 0, color, 3)\n",
    "\n",
    "\n",
    "def draw_circle(\n",
    "    color_image: NDArray[(Any, Any, 3), np.uint8],\n",
    "    center: Tuple[int, int],\n",
    "    color: Tuple[int, int, int] = (0, 255, 255),\n",
    "    radius: int = 6,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draws a circle on the provided image.\n",
    "\n",
    "    Args:\n",
    "        color_image: The color image on which to draw the contour.\n",
    "        center: The pixel (row, column) of the center of the image.\n",
    "        color: The color to draw the circle in BGR format.\n",
    "        radius: The radius of the circle in pixels.\n",
    "    \"\"\"\n",
    "    # cv.circle expects the center in (column, row) format\n",
    "    cv.circle(color_image, (center[1], center[0]), radius, color, -1)\n",
    "\n",
    "\n",
    "def show_color_bgr(blue: int, green: int, red: int) -> None:\n",
    "    \"\"\"\n",
    "    Displays a color specified in the BGR format.\n",
    "    \"\"\"\n",
    "    rectangle = plt.Rectangle((0,0), 50, 50, fc=(red/255, green/255, blue/255))\n",
    "    plt.gca().add_patch(rectangle)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_color_hsv(hue: int, saturation: int, value: int) -> None:\n",
    "    \"\"\"\n",
    "    Displays a color specified in the HSV format\n",
    "    \"\"\"\n",
    "    # Convert from hsv to bgr\n",
    "    hsv = np.array([[[hue, saturation, value]]], np.uint8)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    show_color_bgr(bgr[0][0][0], bgr[0][0][1], bgr[0][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhWBH24Q70wI"
   },
   "source": [
    "Finally, we will create a racecar object.  If this step fails, make sure that `isSimulation` has the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oLdzc2kU70wI",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CvBridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create Racecar\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rc \u001b[39m=\u001b[39m CameraReal()\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mCameraReal.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__bridge \u001b[39m=\u001b[39m CvBridge()\n\u001b[1;32m     13\u001b[0m     \u001b[39m# ROS node\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode \u001b[39m=\u001b[39m ros2\u001b[39m.\u001b[39mcreate_node(\u001b[39m\"\u001b[39m\u001b[39mimage_sub\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CvBridge' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Racecar\n",
    "rc = CameraReal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMXEq2ZS70wI"
   },
   "source": [
    "<a id=\"TakingPhotos\"></a>\n",
    "## 2. Taking Photos\n",
    "In Jupyter Notebook, we can take a photo with the car's camera using `rc.camera.get_color_image_async()`.  Outside of Jupyter Notebook, we must use `rc.camera.get_color_image()` instead.\n",
    "\n",
    "Let's see what the car is currently looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgRdtEPN70wJ",
    "outputId": "ae2c7b6f-544f-44ac-81ef-3b8ba55c9562",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take and display a photo\n",
    "image = rc.get_color_image_async()\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDq2mmvf70wK"
   },
   "source": [
    "Color images are stored as three dimensional numpy arrays:\n",
    "\n",
    "* **0th dimension**: pixel rows, indexed from top to bottom.\n",
    "* **1st dimension**: pixel columns, indexed from left to right.\n",
    "* **2nd dimension**: pixel color values, ordered blue, green, red, each ranging from 0 (none of that color) to 255 (maximum amount of that color).\n",
    "\n",
    "Let's look at the color values of the middle pixel of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "091Pe3iN70wK",
    "outputId": "470b789e-8258-458d-981d-c633676cd20e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate center row and column\n",
    "row = 150\n",
    "col = 200\n",
    "\n",
    "hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
    "# Extract and print blue, green, and red values\n",
    "blue = hsv[row][col][0]\n",
    "green = hsv[row][col][1]\n",
    "red = hsv[row][col][2]\n",
    "\n",
    "print(\"blue:\", blue)\n",
    "print(\"green:\", green)\n",
    "print(\"red:\", red)\n",
    "\n",
    "# Display this color\n",
    "show_color_hsv(blue, green, red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73-Fxz1m70wL"
   },
   "source": [
    "**<span style=\"color:red\">Update `row` and `col` in the following code block to show the pixel that is is 2/3 from the top, 1/4 from the right.</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20N_jqks70wL",
    "outputId": "1c34cb73-5b0a-4e51-bea1-90861f6e7ea6"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate the desired row and column\n",
    "row = 200\n",
    "col = 180\n",
    "\n",
    "# Extract and print blue, green, and red values\n",
    "blue = hsv[row][col][0]\n",
    "green = hsv[row][col][1]\n",
    "red = hsv[row][col][2]\n",
    "\n",
    "print(\"blue:\", blue)\n",
    "print(\"green:\", green)\n",
    "print(\"red:\", red)\n",
    "\n",
    "# Display this color\n",
    "show_color_hsv(blue, green, red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gikw6vaM70wL"
   },
   "source": [
    "<a id=\"ColorFormats\"></a>\n",
    "## 3. Color Formats\n",
    "By default, the images captured by the camera are stored in the blue-green-red (BGR) format.  However, when recognizing objects based on their color, it is far easier to use the hue-saturation-value (HSV) format, in which each channel corresponds to the following:\n",
    "\n",
    "* **Hue** (0 to 180): The color as it appears on a color wheel, ordered as red-orange-yellow-green-blue-purple-red\n",
    "* **Saturation** (0 to 255): The amount of white added to the color.  0 is pure white, and 255 is the pure color without any white added.\n",
    "* **Value** (0 to 255): The amount of black added to the color.  0 is pure black, and 255 is the pure color without any black added.\n",
    "\n",
    "While saturation and value vary with lighting, hue will remain mostly the same regardless of lighting.  By focusing on the hue of the object we are attempting to detect, we can find it even in different lighting environments.\n",
    "\n",
    "We can use the following widgets to experiment with different color values in the BGR and HSV formats.  **<font style=\"color:red\">For both formats, find the values which produce the following colors: orange, pink, dark green, yellow, and gray.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "efefbe95b33a47a1bf821ef5f24e84ce"
     ]
    },
    "id": "24FIKRix70wM",
    "outputId": "68b97c51-f565-40a8-fb09-f223065612f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BGR color\n",
    "widgets.interact(show_color_bgr,\n",
    "                 blue=widgets.IntSlider(0, 0, 255, continuous_update=False),\n",
    "                 green=widgets.IntSlider(0, 0, 255, continuous_update=False),\n",
    "                 red=widgets.IntSlider(0, 0, 255, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "38b111f9e5324894aa40f0d9bcef358d"
     ]
    },
    "id": "L5xUhizv70wM",
    "outputId": "3ab4a870-ffd0-40e9-a360-87db0eb986d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HSV color\n",
    "widgets.interact(show_color_hsv,\n",
    "                 hue=widgets.IntSlider(0, 0, 180, continuous_update=False),\n",
    "                 saturation=widgets.IntSlider(255, 0, 255, continuous_update=False),\n",
    "                 value=widgets.IntSlider(255, 0, 255, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GNCxLt_70wM"
   },
   "source": [
    "<a id=\"Masks\"></a>\n",
    "## 4. Masks\n",
    "Lets work on identifying an object in the car's field of view based on its color.  Specifically, we will isolate the portions of an image which fall within a certain color range by defining upper and lower HSV bounds.  We will use that to create a *mask* - a special type of image which is white in areas to include and black in areas not to include.\n",
    "\n",
    "**<font style=\"color:red\">Finish writing the function `get_mask` below, which takes an image and returns a mask of the areas between hsv_lower and hsv_upper.</font>**  You will likely wish to use the following OpenCV functions:\n",
    "\n",
    "* [`cvtColor`](https://docs.opencv.org/4.2.0/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab): Converts an image from one color format to another, such as from BGR to HSV.\n",
    "* [`inRange`](https://docs.opencv.org/4.2.0/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981): Creates a mask from an image based on a lower and upper color bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkW-TkJq70wN"
   },
   "outputs": [],
   "source": [
    "def get_mask(\n",
    "    image: NDArray[(Any, Any, 3), np.uint8],\n",
    "    hsv_lower: Tuple[int, int, int],\n",
    "    hsv_upper: Tuple[int, int, int]\n",
    ") -> NDArray[Any, Any]:\n",
    "    \"\"\"\n",
    "    Returns a mask containing all of the areas of image which were between hsv_lower and hsv_upper.\n",
    "\n",
    "    Args:\n",
    "        image: The image (stored in BGR) from which to create a mask.\n",
    "        hsv_lower: The lower bound of HSV values to include in the mask.\n",
    "        hsv_upper: The upper bound of HSV values to include in the mask.\n",
    "    \"\"\"\n",
    "    # Convert hsv_lower and hsv_upper to numpy arrays so they can be used by OpenCV\n",
    "    hsv_lower = np.array(hsv_lower)\n",
    "    hsv_upper = np.array(hsv_upper)\n",
    "\n",
    "    # TODO: Use the cv.cvtColor function to switch our BGR colors to HSV colors\n",
    "    hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
    "\n",
    "    # TODO: Use the cv.inRange function to highlight areas in the correct range\n",
    "    mask = cv.inRange(hsv,hsv_lower,hsv_upper)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ln7f-BCo70wN"
   },
   "source": [
    "Let's use `get_mask` to isolate the orange cone(s) in an image.  Place one or more orange cones in the car's line of sight and take a picture with the following block. (If you do not have an orange cone, you can use any object with a distinct color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss0zbune70wO",
    "outputId": "1e8f155a-d2d3-49e8-d8d1-9b357dfb7abd"
   },
   "outputs": [],
   "source": [
    "image = rc.get_color_image_async()\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30aOFz_O70wO"
   },
   "source": [
    "Next, we will use the `get_mask` function to create a mask containing just the cone(s).  At the moment, `hsv_lower` and `hsv_upper` include all possible HSV values, so the mask will contain the entire image.  **<font style=\"color:red\">Tune the values of `hsv_lower` and `hsv_upper` until the mask only includes the cone(s).</font>**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* Use the HSV color widget from [Color Formats](#ColorFormats) to visualize HSV colors.\n",
    "* Copy the image into an image editing software (gimp, paint, etc.) and use the eyedropper (color picker) tool to show the HSV values of the pixels in the cone.\n",
    "* Saturation and value vary a lot with lighting, but hue will remain mostly constant for a given object. Try using a wide range for value and saturation but a tight range for hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvb0-V0o70wO"
   },
   "outputs": [],
   "source": [
    "blue: 26\n",
    "green: 181\n",
    "red: 209\n",
    "    \n",
    "hsv_lower = (26-20, 181-45, 209-30)\n",
    "hsv_upper = (26+20, 181+45, 209+40)\n",
    "\n",
    "mask = get_mask(image, hsv_lower, hsv_upper)\n",
    "show_image(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFrrI4R770wP"
   },
   "source": [
    "We can use this mask as a filter for our original image to only keep portions that were between `hsv_lower` and `hsv_upper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4SNo6FA70wP"
   },
   "outputs": [],
   "source": [
    "masked_image = cv.bitwise_and(image, image, mask=mask)\n",
    "show_image(masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfY_nFl-70wQ"
   },
   "source": [
    "<a id=\"FindingContours\"></a>\n",
    "## 5. Finding Contours\n",
    "\n",
    "Now that we have a mask, we can create outlines called _contours_ around each object in the mask.  We will use these outlines to identify the largest object and calculates its size and position.\n",
    "\n",
    "First, we will use the OpenCV function [`findContours`](https://docs.opencv.org/4.2.0/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0) to create a list of contours around each distinct object in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4Hn476q70wQ"
   },
   "outputs": [],
   "source": [
    "def find_contours(mask: NDArray) -> List[NDArray]:\n",
    "    \"\"\"\n",
    "    Returns a list of contours around all objects in a mask.\n",
    "    \"\"\"\n",
    "    return cv.findContours(mask, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7qQjJJn70wQ"
   },
   "source": [
    "`find_contours` will return a list containing multiple contours if there are multiple distinct objects which fall between `hsv_lower` and `hsv_upper`.  This might occur if there are multiple cones in the image or if there are other objects that have a similar color to the cone(s).\n",
    "\n",
    "Let's write a helper function to identify the largest contour, which we will assume is the closest cone.  This helper function should also ignore contours below a minimum size (such as `30 pixels`), since anything below this size is likely too small to be a cone.\n",
    "\n",
    "**<font style=\"color:red\">Finish writing `get_largest_contour` so that it returns the largest contour larger than `min_area`, or `None` if no such contour exists.</font>**  You will likely wish to use the OpenCV [`contourArea`](https://docs.opencv.org/4.2.0/d3/dc0/group__imgproc__shape.html#ga2c759ed9f497d4a618048a2f56dc97f1) function to find the number of pixels in a contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y-rTlxO70wR"
   },
   "outputs": [],
   "source": [
    "def get_largest_contour(contours: List[NDArray], min_area: int = 30) -> Optional[NDArray]:\n",
    "    \"\"\"\n",
    "    Finds the largest contour with size greater than min_area.\n",
    "\n",
    "    Args:\n",
    "        contours: A list of contours found in an image.\n",
    "        min_area: The smallest contour to consider (in number of pixels)\n",
    "\n",
    "    Returns:\n",
    "        The largest contour from the list, or None if no contour was larger than min_area.\n",
    "    \"\"\"\n",
    "    if len(contours) == 0:\n",
    "        # TODO: What should we return if the list of contours is empty?\n",
    "        return None\n",
    "\n",
    "    # TODO: Return the largest contour, but return None if no contour is larger than min_area\n",
    "    index = 0\n",
    "    biggest = contours[0]\n",
    "    while len(contours)>index:\n",
    "        if contours[index].size>biggest.size:\n",
    "            biggest = contours[index]\n",
    "        index = index +1\n",
    "    if biggest.size<min_area:\n",
    "        return None\n",
    "    return biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU_zGZEz70wR"
   },
   "source": [
    "Let's try it out.  The following code block uses `find_contours` and `get_largest_contour` to find the largest contour and draw it on the image.  We should now see a green outline surrounding the closest cone in our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThS23wdx70wR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the largest contour\n",
    "contours = find_contours(mask)\n",
    "largest_contour = get_largest_contour(contours)\n",
    "\n",
    "# Draw it on the image\n",
    "image_copy = np.copy(image)\n",
    "draw_contour(image_copy, largest_contour)\n",
    "show_image(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-iZE3qj70wS"
   },
   "source": [
    "<a id=\"ContourCenter\"></a>\n",
    "## 6. Contour Center\n",
    "\n",
    "One advantage of contours is that we can use them to easily calculate the center of an object.  Specifically, we will use the contour's [_Moments_](https://en.wikipedia.org/wiki/Image_moment), which are weighted averages of the pixels in the contour.  We can calculate the moment $M_{ij}$ with the following formula:\n",
    "\n",
    "```\n",
    "def moment(i, j):\n",
    "    sum = 0\n",
    "    for pixel in contour:\n",
    "        sum += pixel.x_position ** i + pixel.y_position ** j\n",
    "    return sum\n",
    "```\n",
    "\n",
    "To calculate contour center, we will use the following moments:\n",
    "\n",
    "* $M_{00}$: The number of pixels in the contour.\n",
    "* $M_{10}$: The sum of how far to the right each pixel in the contour is.\n",
    "* $M_{01}$: The sum of how far down each pixel in the contour is.\n",
    "\n",
    "Using the [center of mass equation](https://en.wikipedia.org/wiki/Center_of_mass), $\\frac{M['m10']}{M['m00']}$ gives us the average horizontal position (column) of the contour, and $\\frac{M['m01']}{M['m00']}$ gives us the average vertical position (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHE-PSk070wS"
   },
   "outputs": [],
   "source": [
    "def get_contour_center(contour: NDArray) -> Optional[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Finds the center of a contour from an image.\n",
    "\n",
    "    Args:\n",
    "        contour: The contour of which to find the center.\n",
    "\n",
    "    Returns:\n",
    "        The (row, column) of the pixel at the center of the contour, or None if the contour is empty.\n",
    "    \"\"\"\n",
    "    # Ask OpenCV to calculate the contour's moments\n",
    "    M = cv.moments(contour)\n",
    "\n",
    "    # Check that the contour is not empty\n",
    "    if M[\"m00\"] <= 0:\n",
    "        return None\n",
    "\n",
    "    # Compute the center of mass of the contour\n",
    "    center_row = round(M[\"m01\"] / M[\"m00\"])\n",
    "    center_column = round(M[\"m10\"] / M[\"m00\"])\n",
    "\n",
    "    return (center_row, center_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_c0f2et70wS"
   },
   "source": [
    "To see if this worked, we will draw a dot at this calculated center point. We should now see a yellow dot at the center of the cone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhKCqEqR70wT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "center = get_contour_center(largest_contour)\n",
    "\n",
    "# Draw a circle at the contour center\n",
    "draw_circle(image_copy, center)\n",
    "show_image(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47Opk9ny70wZ"
   },
   "source": [
    "<a id=\"ContourArea\"></a>\n",
    "## 7. Contour Area\n",
    "\n",
    "When writing `get_largest_contour`, you likely used the OpenCV [`contourArea`](https://docs.opencv.org/4.2.0/d3/dc0/group__imgproc__shape.html#ga2c759ed9f497d4a618048a2f56dc97f1) function.  Contour area is also helpful for calculating how far an object is from the camera, since the closer an object is, the more pixels it will take up on the screen.\n",
    "\n",
    "In this section, we will measure the area of the cone at different distances from the car. Using previous examples from this notebook, **<font style=\"color:red\">update the following code block to take a photo, find the largest contour, print the contour area, and display the image with the contour.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcIlS5Kl70wZ"
   },
   "outputs": [],
   "source": [
    "rc = CameraReal()\n",
    "# TODO: Take a photo\n",
    "image = rc.get_color_image_async()\n",
    "show_image(image)\n",
    "\n",
    "# blue line threshold\n",
    "hsv_lower = (85, 155, 230)\n",
    "hsv_upper = (100, 200, 255)\n",
    "\n",
    "#yellow cone thresold\n",
    "hsv_lower = (26-20, 181-45, 209-30)\n",
    "hsv_upper = (26+20, 181+45, 209+40)\n",
    "\n",
    "mask = get_mask(image, hsv_lower, hsv_upper)\n",
    "contours = find_contours(mask)\n",
    "largest_contour = get_largest_contour(contours)\n",
    "\n",
    "# TODO: Calculate and print the largest contour's area\n",
    "print(largest_contour.size/2)\n",
    "\n",
    "# TODO: Display the image with the contour drawn on top\n",
    "image_copy = np.copy(image)\n",
    "draw_contour(image_copy, largest_contour)\n",
    "show_image(image_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD1nYCLW70wZ"
   },
   "source": [
    "**<font style=\"color:red\">Measure the cone contour area when the cone is the following distances away from the car: 40 cm, 80 cm, 120 cm, 160 cm, 200 cm.  Update the entries in `data` with your results.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utGlCdqw70wZ"
   },
   "outputs": [],
   "source": [
    "# Datapoints in the format (cone distance from car in cm, number of pixels in contour)\n",
    "# TODO: Fill is contour area (currently 0)\n",
    "data = [\n",
    "    (40, 208.0),\n",
    "    (80, 119.0),\n",
    "    (120, 56.0),\n",
    "    (160, 34.0),\n",
    "    (200, 21.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pAqVvdM70wa"
   },
   "source": [
    "Let's plot this data to see the relationship between distance and cone area.  **How would you describe this relationship?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1t6SrL6Z70wa"
   },
   "outputs": [],
   "source": [
    "# Plot data in a scatter plot\n",
    "data_t = np.transpose(data)\n",
    "plt.scatter(data_t[0], data_t[1])\n",
    "plt.title(\"Relationship between Distance and Contour Area\")\n",
    "plt.xlabel(\"Cone distance (cm)\")\n",
    "plt.ylabel(\"Contour area (pixels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0-NP7eU70wa"
   },
   "source": [
    "<a id=\"ProcessingNumericValues\"></a>\n",
    "## 8. Processing Numeric Values\n",
    "\n",
    "At a high level, many robotics tasks can be summarized as follows:\n",
    "\n",
    "1. Receive raw inputs from one or more sensors (images, IMU data, LIDAR scans, etc.).\n",
    "2. Convert these raw inputs into numeric values (contour center, contour area, center distance, current velocity, etc.).\n",
    "3. Use these numeric values to calculate output values (speed, angle, etc.).\n",
    "4. Send these output values to controllers (motors, etc.).\n",
    "\n",
    "In the previous sections, we saw two examples of step 2 when we turned a color image (a raw input) into the a center point and a contour area (numeric values).  In this section, we will build up two useful tools for step 3: turning these numeric values into output values.\n",
    "\n",
    "### Clamp\n",
    "Frequently, an output value must fall within a certain range.  For example, the speed and angle sent to `rc.drive.set_speed_angle()` must be in the range $[-1, 1]$.  *Clamping* refers to the process of forcing a value into a particular range.  If the value is less than the minimum, it receives the minimum value, and if the value is greater than the maximum, it receives the maximum value.\n",
    "\n",
    "For example, clamp should return the following outputs:\n",
    "\n",
    "* `clamp(3, 0, 10) = 3`: $3$ is between $0$ and $10$, so no change is necessary.\n",
    "* `clamp(-2, 0, 10) = 0`: $-2$ is less than $0$, so we saturate it at the minimum value.\n",
    "* `clamp(11, 0, 10) = 10`: $11$ is greater than $10$, so we saturate it at the maximum value.\n",
    "\n",
    "**<font style=\"color:red\">Implement this behavior in the `clamp` function below.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRC1PL-j70wa"
   },
   "outputs": [],
   "source": [
    "def clamp(value: float, vmin: float, vmax: float) -> float:\n",
    "    \"\"\"\n",
    "    Clamps a value between a minimum and maximum value.\n",
    "\n",
    "    Args:\n",
    "        value: The input to clamp.\n",
    "        vmin: The minimum allowed value.\n",
    "        vmax: The maximum allowed value.\n",
    "\n",
    "    Returns:\n",
    "        The value saturated between vmin and vmax.\n",
    "    \"\"\"\n",
    "    \n",
    "    if value > vmax:\n",
    "        value = vmax\n",
    "    elif value < vmin:\n",
    "        value = vmin\n",
    "        \n",
    "    return value\n",
    "    # TODO: Make sure that value is between min and max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp4ueNqx70wa"
   },
   "source": [
    "Let's test out our `clamp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6Kv01Z270wb"
   },
   "outputs": [],
   "source": [
    "# Testing clamp\n",
    "assert clamp(3, 0, 10) == 3\n",
    "assert clamp(-2, 0, 10) == 0\n",
    "assert clamp(11, 0, 10) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_92rqzj470wb"
   },
   "source": [
    "We can use the following widget to experiment with different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVW3LB-k70wb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "widgets.interact(clamp,\n",
    "                 value=widgets.FloatSlider(0, min=-2, max=2, step=0.1),\n",
    "                 vmin=widgets.fixed(-1),\n",
    "                 vmax=widgets.fixed(1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff6_jeob70wb"
   },
   "source": [
    "### Remap Range\n",
    "\n",
    "We often wish to take action based on where an input value falls within a given range.  For example, we may set the car's angle based on the x-coordinate of a contour center, or set the car's speed based on the center distance of a depth image.  One approach is to *remap* the input value from a range of possible input values to a range of possible output values.\n",
    "\n",
    "For example, suppose we are remapping the range $[0, 1]$ to $[-1, 1]$. The value $0$ (the old min) becomes $-1$ (the new min), $0.5$ (the old midpoint) becomes $0$ (the new midpoint), and the value $1$ (the old max) becomes $1$ (the new max).\n",
    "\n",
    "The ranges do not necessarily need to be \"in order\".  For example, suppose that we want small numbers to become large and vice versa.  We can achieve this by flipping the new range, such as remapping $[0, 10]$ to $[10, 0]$.  For example, $2$ will become $8$, $0$ will become $10$, and $6$ will become $4$.  We can even use values outside of this initial range: $20$ will become $-10$, and $-2$ will become $12$.\n",
    "\n",
    "**<font style=\"color:red\">In `remap_range`, write code to remap `val` from an old range to a new range.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH9O122O70wb"
   },
   "outputs": [],
   "source": [
    "def remap_range(\n",
    "    val: float,\n",
    "    old_min: float,\n",
    "    old_max: float,\n",
    "    new_min: float,\n",
    "    new_max: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Remaps a value from one range to another range.\n",
    "\n",
    "    Args:\n",
    "        val: A number form the old range to be rescaled.\n",
    "        old_min: The inclusive 'lower' bound of the old range.\n",
    "        old_max: The inclusive 'upper' bound of the old range.\n",
    "        new_min: The inclusive 'lower' bound of the new range.\n",
    "        new_max: The inclusive 'upper' bound of the new range.\n",
    "\n",
    "    Note:\n",
    "        min need not be less than max; flipping the direction will cause the sign of\n",
    "        the mapping to flip.  val does not have to be between old_min and old_max.\n",
    "    \"\"\"\n",
    "    \n",
    "    diff = old_max - old_min\n",
    "    percent = val/diff\n",
    "    new_val = percent * (new_max-new_min)\n",
    "    new_val += new_min\n",
    "    \n",
    "    return new_val\n",
    "    # TODO: remap val to the new range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5htkwcM70wb"
   },
   "source": [
    "Let's test our `remap range` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df6t46LS70wc"
   },
   "outputs": [],
   "source": [
    "# Testing remap_range\n",
    "assert remap_range(5, 0, 10, 0, 50) == 25\n",
    "assert remap_range(5, 0, 20, 1000, 900) == 975\n",
    "assert remap_range(2, 0, 1, -10, 10) == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL5G1if-70wc"
   },
   "source": [
    "We can use the following widget to experiment with different inputs.  **Find three different input combinations which yield the output $2.0$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBPFWTXR70wc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interact(remap_range,\n",
    "                 val=widgets.FloatSlider(0, min=-10, max=10, step=0.1),\n",
    "                 old_min=widgets.FloatSlider(0, min=-10, max=10, step=0.1),\n",
    "                 old_max=widgets.FloatSlider(5, min=-10, max=10, step=0.1),\n",
    "                 new_min=widgets.FloatSlider(-1, min=-10, max=10, step=0.1),\n",
    "                 new_max=widgets.FloatSlider(1, min=-10, max=10, step=0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16QhTgJh70wc"
   },
   "source": [
    "<a id=\"ProportionalControl\"></a>\n",
    "## 9. Proportional Control\n",
    "\n",
    "A frequent goal in robotics is to maintain a desired state.  For example, we may wish for the car to stay 30 cm away from a cone or maintain a speed of 1.0 m/s.  When the current state differs from the desired state, we use controllers on the car (such as throttle or steering) to move toward the desired state.  \n",
    "\n",
    "One of the simplest ways to maintain steady state is [proportional control](https://en.wikipedia.org/wiki/Proportional_control).  This strategy applies a change that is *proportional* to the difference between the current value and the desired value.  For example, suppose that we wish to maintain a room at 20.0 degrees Celsius. For every degree the actual temperature differs from this desired value, we will apply 100 units of heating/cooling.  That is:\n",
    "* If the current temperature is $20.0$ degrees, we are at the desired temperature so apply $0$ units of heating.\n",
    "* If the current temperature is $19.7$ degrees, we apply $30$ units of heating\n",
    "* If the current temperature is $21.7$ degrees, we apply $-170$ units of heating.\n",
    "\n",
    "We can use `remap_range` to perform proportional control calculations by providing the following inputs:\n",
    "* `val`: The input variable (ex: current temperature)\n",
    "* `old_min`: One potential value for the input variable (ex: desired temperature)\n",
    "* `old_max`: Another potential value for the input variable (ex: 1 degree less than the desired temperature)\n",
    "* `new_min`: The desired output when the input is `old_min` (ex: no heating/cooling)\n",
    "* `new_max`: The desired output when the input is `old_max` (ex: 100 units of heating)\n",
    "\n",
    "Notice that `min` and `max` are not boundaries and do not need to be in order.  They are simply two reference points to characterize the linear relationship.\n",
    "\n",
    "**<font style=\"color:red\">Experiment with different values of `current_temp`.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4eGl4bL70wd"
   },
   "outputs": [],
   "source": [
    "DESIRED_TEMP = 20\n",
    "current_temp = 19.7 # TODO: experiment with different values\n",
    "\n",
    "# Use remap_range to calculate the units of heating to apply\n",
    "heating = remap_range(current_temp, DESIRED_TEMP, DESIRED_TEMP - 1, 0, 100)\n",
    "print(f\"{heating:.1f} units of heating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbQ1cNwL70wd"
   },
   "source": [
    "If our output must fall within a certain range, we can use `clamp` to enforce this range.  For example, suppose that our thermostat can apply a maximum of 500 units of heating or cooling.  The following code block uses `clamp` to enforce this restriction.\n",
    "\n",
    "**<font style=\"color:red\">See what happens when `current_temp` is below 15 degrees or above 25 degrees.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GujIeJI70wd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clamped_heating = clamp(heating, -500, 500)\n",
    "print(f\"{clamped_heating:.1f} units of heating (after clamping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KolQwMb570wd"
   },
   "source": [
    "For our final challenge, we will use proportional control to determine the front wheel angle based on the position of a cone in the color image.  Specifically, we will steer the car towards the cone with the following steps:\n",
    "1. Take a color photo.\n",
    "2. Mask the image based on color and identify the largest contour, which we assume is the cone.\n",
    "3. Find the center of this contour.\n",
    "4. Print the color image with the contour and center drawn on top.\n",
    "5. Convert the column of the center to an angle using proportional control. That is, the car should turn to the left proportional to how far the cone is to the left, or turn to the right proportional to how far the cone is to the right.\n",
    "\n",
    "**<font style=\"color:red\">Perform these steps in the following block of code to calculate an angle.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhU_d9b870we"
   },
   "outputs": [],
   "source": [
    "rc = CameraReal()\n",
    "# TODO: Take a photo\n",
    "image = rc.get_color_image_async()\n",
    "show_image(image)\n",
    "\n",
    "# blue line threshold\n",
    "hsv_lower = (85, 155, 230)\n",
    "hsv_upper = (100, 200, 255)\n",
    "\n",
    "#yellow cone thresold\n",
    "hsv_lower = (26-20, 181-45, 209-30)\n",
    "hsv_upper = (26+20, 181+45, 209+40)\n",
    "\n",
    "mask = get_mask(image, hsv_lower, hsv_upper)\n",
    "contours = find_contours(mask)\n",
    "largest_contour = get_largest_contour(contours)\n",
    "\n",
    "center = get_contour_center(largest_contour)\n",
    "print(center)\n",
    "\n",
    "# Draw a circle at the contour center\n",
    "draw_circle(image_copy, center)\n",
    "show_image(image_copy)\n",
    "\n",
    "print()\n",
    "# TODO: Find the center of the contour and print its coordinates\n",
    "\n",
    "\n",
    "# TODO: Display the image with the contour and center drawn on top\n",
    "\n",
    "\n",
    "# TODO: Use the contour center to calculate and print an angle in the range [-1, 1]\n",
    "print(remap_range(center[0], 0, 640, -1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9AOH0VS70we"
   },
   "source": [
    "You are now ready to begin using the color camera to follow lines in `lab2a.py`. Good luck, and don't be afraid to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
